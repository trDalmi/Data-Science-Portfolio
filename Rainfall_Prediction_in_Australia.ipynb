{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2jaMsm9+OZ6SMhEY0Rf75",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trDalmi/Data-Science-Portfolio/blob/main/Rainfall_Prediction_in_Australia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rainfall in Australia\n",
        "\n",
        "Implementation of Logistic Regression with Python and Scikit-Learn to build a classifier to predict whether or not it will rain tommorow in Australia. Training a binary classification model using Logistic Regresssion.\n",
        "\n",
        "## Table of Contents:\n",
        "The table of contents for this project is as follow:-\n",
        "\n",
        "1. The Problem Statement\n",
        "2. Import Libraries\n",
        "3. Import Dataset\n",
        "4. Exploratory Data Analysis\n",
        "*   4.1 View dimensions of dataset\n",
        "*   4.2 Preview the dataset\n",
        "*   4.3 View column names\n",
        "*   4.4 Drop variables\n",
        "*   4.5 View summary of dataset\n",
        "*   4.6 View statistical properties of datset\n",
        "\n",
        "5. Univariate Anaysis.\n",
        "*   5.1 Explore RainTommorow target variable.\n",
        "*   5.2 Finding of Univariate Analysis.\n",
        "6. Bivariate Analysis\n",
        "*   6.1 Types of varibles\n",
        "*   6.2 Explore Categorical variables\n",
        "*   6.3 Summary of Categorical variables.\n",
        "*   6.4 Explore problems within categorical varibales.\n",
        "*   6.5 Explore numerical variables.\n",
        "*   6.6 Summary of numerical variables.\n",
        "*   6.7 Exolore problems within numerical variables.\n",
        "7. Multivariate Analysis.\n",
        "*   7.1 Heat Map\n",
        "*   7.2 Pair Plot.\n",
        "8. Declare feature vector and target variable\n",
        "9. Split data into trainig and test set.\n",
        "10. Feature Engineering\n",
        "*   10.1 Engineering missing values in numerical variables.\n",
        "*   10.2 Engineering missing values in categorical variables.\n",
        "*   10.3 Engineering outliers in numerical variables.\n",
        "*   10.4 Encode categorical variables.\n",
        "11. Feature Scaling\n",
        "12. Model training\n",
        "13. Predict results\n",
        "14. Check accuracy score\n",
        "*   14.1 Compare train-set and test-set accuracy.\n",
        "*   14.2 Check for overfitting and underfitting\n",
        "*   14.3 Compare model accuracy with null accuracy.\n",
        "15. Confusion Matrix\n",
        "16. Classification metrices.\n",
        "*   16.1 Classification report\n",
        "*   16.2 Classification accuracy.\n",
        "*   16.3 Classification error.\n",
        "*   16.4 Precision\n",
        "*   16.5 Recall\n",
        "*   16.6 True Positive Rate\n",
        "*   16.7 False Positive Rate\n",
        "*   16.8 Specificity\n",
        "*   16.9 f1-score\n",
        "*   16.10 Support\n",
        "17. Adjust the threshold level.\n",
        "18. ROC-AUC\n",
        "19. Recursive Feature Elimination with cross(RFECV)\n",
        "20. k-Fold Cross-Validation\n",
        "21. Hyperparameter Optimization using GridSearch CV\n",
        "22. Result and Conclusion\n",
        "23. References.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-HbMYW0q_vOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The Problem Statement\n",
        "\n",
        "In this project, we will try to answer the question that whether or not it will rain tommorow in Australia.\n",
        "We will implement Logistic Regression with Python and Scikit-Learn\n",
        "\n",
        "To answer the question, we build a classifier to predict whether or not it will rain tomorrow in Australia. We train a binary classification model using Logistic Regression. I have used the Rain in Australia dataset for this project.\n",
        "## 2. Import Librabies\n",
        "The first step in building the model is to import the necessary libraries."
      ],
      "metadata": {
        "id": "_v28kPcVGeTS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3D40VqJ_rFI"
      },
      "outputs": [],
      "source": [
        "import numpy as np # LINEAR ALGEBRA\n",
        "import pandas as pd # DATA PROCESSING, CSV FILE I/O\n",
        "\n",
        "# importing libraries for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import Dataset\n",
        "The next step is to import the dataset."
      ],
      "metadata": {
        "id": "cvBvm3OpJ6QP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "J10q8uBTJ5hj",
        "outputId": "b0eaa790-a630-469a-ea27-8e55f31e9a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Datasets/weatherAUS.csv\"\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "3iRV7aumKHJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Exploratory data analysis\n",
        "*   We have imported the data.\n",
        "*   Now, it time to explore the data to gain insights about it.\n",
        "\n",
        "### View Dimesions of dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "HOon6yZMKqaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "fmrZoVkpKpOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the data contains 142193 instances and 24 variables in the dataset.\n",
        "### Preview the dataset."
      ],
      "metadata": {
        "id": "A1CkcaRGK-85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "CKPwiY2hK88V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View Column names"
      ],
      "metadata": {
        "id": "d8Av3dGILWmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "NdIhXg-oLMpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset contains detailed meteorological information recorded in Albury, Australia, for a specific period. It includes 24 attributes for each day, capturing various aspects of weather. These attributes include:\n",
        "\n",
        "\n",
        "*   DATE : The date of the weather observation.\n",
        "*   Location: The location where the observation was made.\n",
        "*   MinTemp: Minimum temperature of the day (in degree Celsius)\n",
        "*   MaxTemp: Maximum temperature of the day (in degree Celsius)\n",
        "*   Rainfall: Total rain recorded for the day in mm.\n",
        "*   Evapration: Total evaporation recorded for the day in mm.\n",
        "*   Sunshine: Total hours of sunshine recorded for the day.\n",
        "*   WindGustDir: The direction of the strongest wind gust during the day.\n",
        "*   WindGustSpeed: The speed of the strongest wind gust during the day.\n",
        "*   WindDir9am: The wind direction at 9am.\n",
        "*   WindDir3pm: The wind direction at 3pm.\n",
        "*   WindSpeed9am: The wind speed at 9am (in kilometers per hours).\n",
        "*   WindSpeed3pm: The wind speed at 3pm (in kilometers per hours).\n",
        "*   Humidity9am: The humidity level at 9am (percentage)\n",
        "*   Humidity3pm: The humidity level at 3pm (percentage)\n",
        "*   Cloud9am: The cloud cover at 9am (measured in oktas).\n",
        "*   Cloud3pm: The cloud cover at 3pm (measured in oktas).\n",
        "*   Temp9am: The temperature at 9am (in degree Celsius).\n",
        "*   Temp3pm: The temperature at 3pm (in degree Celsius).\n",
        "*   RainToday: Indicator of whether it rainged today (YES or NO).\n",
        "*   RISK_MM: The amount of rain(in millimeters) recorded for the next day.\n",
        "*   RainTomorrow: Indicator of whether it rained the next day (Yes or NO).'\n",
        "\n",
        "This dataset provides comprehensive insights into daily weather patterns and variations, aiding in climate analysis and prediction.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "adHSrYYtLhLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "VsLqXHm0Lcm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comment:\n",
        "*   We can see that the dataset contains mixture of categorical and numerical values.\n",
        "*   Categorical variables have data type **Object**.\n",
        "*   Numerical variables have data type **Float64**.\n",
        "*   Also, there are some missing values in the dataset as all the  attributes does not have the same count of values.\n",
        "\n",
        "Counting the null values for each attributes:\n",
        "\n"
      ],
      "metadata": {
        "id": "HGbzSHpkRDgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "l0WcPF31QaLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at null values. (as we can see the dataframe has a lot of null values)\n",
        "\n",
        "### View Statistical Properties of dataset."
      ],
      "metadata": {
        "id": "BucnBDxEQr0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "-cd8zXz5QgWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important points to note:\n",
        "*   The above commad df.describe() helps us to view the statistical properties of numerical variables. It excludes character variables.\n",
        "*   If we want to view the statistical properties of character variables, we should run the following command-\n",
        "    df.describe(include = ['object'])\n",
        "*   If we want to view the statistical properties of all the variables, we should run the following command-\n",
        "    df.describe(include = \"all\")\n"
      ],
      "metadata": {
        "id": "w2RbFYjsSMfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include = \"all\")"
      ],
      "metadata": {
        "id": "Mi1cgl--SK65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Univariate Analysis\n",
        "#### Explore **RainTomorrow** target variable\n",
        "Check for missing values"
      ],
      "metadata": {
        "id": "bzQpKPk3TTHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].isnull().sum()"
      ],
      "metadata": {
        "id": "RcfJs0SYTERp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are no missing values in the RainTomorrow target variable.\n",
        "#### View the number of unique values."
      ],
      "metadata": {
        "id": "sdno-4zKWSjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].nunique()"
      ],
      "metadata": {
        "id": "vDxjWq3gWoEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### View the unique values."
      ],
      "metadata": {
        "id": "TGCBbUGQXH-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].unique()"
      ],
      "metadata": {
        "id": "RMiSuzjdWA2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the number of unique values in RainTommorow varibale is 2. And they are YES and NO.\n",
        "#### View the frequency distribution of values."
      ],
      "metadata": {
        "id": "2FobbIV6WsaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].value_counts()"
      ],
      "metadata": {
        "id": "pfguqWdrWdV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### View percentage of frequency distribution of values."
      ],
      "metadata": {
        "id": "VbTZxNkbXfhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].value_counts()/len(df)"
      ],
      "metadata": {
        "id": "mQzk03fHXX48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comment\n",
        "*  We can see that out of the total number of RainTomorrow vlaues, NO appears 77.58% times and YES appears 22.42% times.\n",
        "\n",
        "?? Can we say that the data is unbalanced and will make biased predictions."
      ],
      "metadata": {
        "id": "Zf-wRugrXzef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,6))\n",
        "sns.countplot(x = df['RainTomorrow'],palette = \"Set1\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5tPdSOCIXrvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Interpretation\n",
        "*  The above univariate plot confirms our findings that-\n",
        "  *  The NO variable have 110316 entries, and\n",
        "  * The Yes variable have 31877 entries.\n",
        "\n",
        "Henceforth we can say that the the data is unbalanced.\n",
        "\n",
        "### Findings of Univariate Analysis:\n",
        "*  The number of unique values in RainTomorrow variable is 2.\n",
        "\n",
        "*  The two unique values are No and Yes.\n",
        "\n",
        "*  Out of the total number of RainTomorrow values, No appears 77.58% times and Yes appears 22.42% times.\n",
        "\n",
        "*  The univariate plot confirms our findings that â€“\n",
        "\n",
        "   * The No variable have 110316 entries, and\n",
        "\n",
        "  * The Yes variable have 31877 entries."
      ],
      "metadata": {
        "id": "IQ4X8l5WZxZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Bivariate Analysis\n",
        "\n",
        "### Types of Varibales\n",
        "\n",
        "In this section, We will segregate the dataset into categorical and numerical variables. There are a micture of categorical and numerical varibales in the dataset. Categorical varibale have data type object. Numerical varibales have data type float64.\n",
        "\n",
        "First of all, I will find the cateforical varibales.\n",
        "\n",
        "\n",
        "\n",
        "#### Explore Catergorical Variables"
      ],
      "metadata": {
        "id": "m4Fg6s5xaobC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical = [var for var in df.columns if df[var].dtype == 'object']\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "print('The categorical variables are :\\n\\n', categorical)"
      ],
      "metadata": {
        "id": "-y3Fkuhhbzx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the categorical varibales\n",
        "df[categorical].head()"
      ],
      "metadata": {
        "id": "hMCOkeEZeB_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Summary of categorical varibales\n",
        "*  There is a date varibale. It is denoted by Date column.\n",
        "*  There are 6 categorical variables. These are given by Location, WindGustDir, WindDir9am, WindDir3pm, RainToday, and RainTomorrow.\n",
        "*  There are two binary categorical varibales - RainToday and RainTomorrow.\n",
        "*  RainTomorrow is the target variable.\n",
        "\n",
        "#### Explore the problems within categorical variables\n",
        "First, I will explore the categorical variables.\n",
        "#### Missing values in categorical variables"
      ],
      "metadata": {
        "id": "h7yPzl1hgzOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the missing values in categorical variables\n",
        "df[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "eCVhPQARgZ6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print categorical variables containing missing values\n",
        "cat1 = [var for var in categorical if df[var].isnull().sum()!=0]\n",
        "print(df[cat1].isnull().sum())"
      ],
      "metadata": {
        "id": "KeYlb8HPh-KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are only 4 categorical varibales in dataset which contains missing values. These are WindGustDir, WindDir9am, WindDir3pm, and RainToday.\n",
        "\n",
        "#### Frequency count of categorical variables.\n",
        "Now, lets check the frequenct counts of categorical varibales."
      ],
      "metadata": {
        "id": "UenvAOgrjWhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for var in categorical:\n",
        "  print(df[var].value_counts())"
      ],
      "metadata": {
        "id": "dBuHqHeNi6Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view frequency distribution of categorical variables\n",
        "for var in categorical:\n",
        "  print(df[var].value_counts()/(len(df)))"
      ],
      "metadata": {
        "id": "5GaQxZ3uj0PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Number of labels: cardinality\n",
        "The number of labels within a categorical varibales is known as cardinality. A high number of labels within a varbale is known as high cardinality. High cardinality may pose some serious problems in the machine learning model. SO, lets check for high cardinality."
      ],
      "metadata": {
        "id": "mnjO38pMkUzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for cardinality in categorical variables\n",
        "for var in categorical:\n",
        "  print(var, 'contains', df[var].nunique(), 'labels')"
      ],
      "metadata": {
        "id": "zuMhNR1HkMOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there is a **Date** variable which needs to be preprocesses. I will do preprocessing in the following section. Here rather than consider the date as for date we will generalise it a months or week more minised lens.\n",
        "\n",
        "\n",
        "All the other variables contain relatively smaller number of variables."
      ],
      "metadata": {
        "id": "KOOVQjielEWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering of Date variable"
      ],
      "metadata": {
        "id": "v3UDKZlcmalg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Date'].dtype"
      ],
      "metadata": {
        "id": "LAezom5Ak_NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the data type of Date variable is object. I will parse the date currently coded as object into datetime format."
      ],
      "metadata": {
        "id": "YOOWg8sWCesd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the dates, currently coded as strings, into datitme format\n",
        "df['Date'] = pd.to_datetime(df['Date'])"
      ],
      "metadata": {
        "id": "IOrbTuvymhqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract year from the date\n",
        "df['Year'] = df['Date'].dt.year.astype(np.float64)\n",
        "df['Year'].head()"
      ],
      "metadata": {
        "id": "xur3xIjUCxPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Month from the date\n",
        "df['Month'] = df['Date'].dt.month.astype(np.float64)\n",
        "df['Month'].head()"
      ],
      "metadata": {
        "id": "z5FGZ-RwDWyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Day from the date\n",
        "df['Day'] = df['Date'].dt.day.astype(np.float64)\n",
        "df['Day'].head()"
      ],
      "metadata": {
        "id": "sCiOI-9jDnSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "S5i6zbuwDqoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are three additional columns created from the Date variable. Now, I will drop the original Date variable from the dataset"
      ],
      "metadata": {
        "id": "p5-lbDg-EZCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop('Date', axis =1 , inplace = True)\n",
        "df.describe(include = 'all')"
      ],
      "metadata": {
        "id": "riz-T1siD2OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can see that the Date variable has been removed from the dataset.\n",
        "\n",
        "### Explore Categorical Variables one by one\n",
        "Now, I will explore the categorical varibales one by one."
      ],
      "metadata": {
        "id": "aU2D20IQEtBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fin categorical variables\n",
        "categorical = [var for var in df.columns if df[var].dtype == 'object']\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "print('The categorical variables are :\\n\\n', categorical)"
      ],
      "metadata": {
        "id": "1XPxKlPQD4_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that there are 6 categorical varibales in the dataset. The Date varibale has been removes. First, I will check missing values in categorical varibales."
      ],
      "metadata": {
        "id": "Yj6l_wvZFSMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[categorical].isnull().sum()"
      ],
      "metadata": {
        "id": "VQ1_TZVWFLyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that WindGustDir, WindDir9am, WindDir3pm, RainToday variables contain missing values. I will explore these variables one by one.\n",
        "\n",
        "### Explore Location variable"
      ],
      "metadata": {
        "id": "3Rh9RjAwFmjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print the lables of the location variables\n",
        "df['Location'].nunique()"
      ],
      "metadata": {
        "id": "VgD47slnFgy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 49 different labels in the location. lets see what are those:\n"
      ],
      "metadata": {
        "id": "ZIc9YnZEGDxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Location'].unique()"
      ],
      "metadata": {
        "id": "Vk5UcGHrF_0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Location'].value_counts()"
      ],
      "metadata": {
        "id": "4J1xwfTkGKPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.Location.isna().sum()"
      ],
      "metadata": {
        "id": "EzxnvVI2J3Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that there are no missing values in the Location variables"
      ],
      "metadata": {
        "id": "wJYDKILPKUUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindGustDir'].nunique()"
      ],
      "metadata": {
        "id": "m056qFBTKTYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 16 different wind gust direction in the dataframe which are as follows:"
      ],
      "metadata": {
        "id": "wSUUbjjcKinm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindGustDir'].unique()"
      ],
      "metadata": {
        "id": "ymsATcwoKh1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindGustDir'].value_counts()"
      ],
      "metadata": {
        "id": "-GLRU3X6MP0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.WindGustDir.isna().sum()"
      ],
      "metadata": {
        "id": "4_ClAr5wKqhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 9330 null values in this variable column\n",
        "\n",
        "### One Hot Ecoding\n"
      ],
      "metadata": {
        "id": "vJ4sUtXFK1GP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sum the number of 1s per boolean vaiable over the rows of the dataset\n",
        "# it will tell us how many observations we have for each category\n",
        "\n",
        "pd.get_dummies(df.WindGustDir, dummy_na = True).sum()"
      ],
      "metadata": {
        "id": "UMTcvCwnK0LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By doing one hot endoing and finding out the total nan values will also give us the null vlaues of the variables.\n",
        "\n",
        "### Explore WindDir9am variable"
      ],
      "metadata": {
        "id": "LXzph-pHNUub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir9am'].nunique()"
      ],
      "metadata": {
        "id": "qvxiU9E1OB2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 16 unique values of winddir9am, those are as follows:"
      ],
      "metadata": {
        "id": "X2L_ZZSxOeVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir9am'].unique()"
      ],
      "metadata": {
        "id": "BwBYL5irOTCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir9am'].value_counts()"
      ],
      "metadata": {
        "id": "rpIH3EqdOneJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir9am'].isna().sum()"
      ],
      "metadata": {
        "id": "Oyj_jrs2OvO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 10013 null values\n"
      ],
      "metadata": {
        "id": "mEJRcoW9O48F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df['WindDir9am'],dummy_na = True).sum()"
      ],
      "metadata": {
        "id": "uQl4S6evO3qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore WindDir3pm Variable"
      ],
      "metadata": {
        "id": "n3UULpm9PZO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir3pm'].nunique()"
      ],
      "metadata": {
        "id": "Y1Z4SYSyO_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 16 lables of WindDir3pm, and those are as follows:"
      ],
      "metadata": {
        "id": "Kuyb3s_wPjnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir3pm'].unique()"
      ],
      "metadata": {
        "id": "iu0cTtrzPivM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir3pm'].value_counts()"
      ],
      "metadata": {
        "id": "pkBXWhDFPu1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['WindDir3pm'].isna().sum()"
      ],
      "metadata": {
        "id": "PDDaZ570Pwad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above two operations can be done in one single operations using one hot encoding as follows:\n"
      ],
      "metadata": {
        "id": "PcQmd4vNP4o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df['WindDir3pm'], dummy_na = True).sum()"
      ],
      "metadata": {
        "id": "MlzwiwAyP3mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3778 null values in the WindDir3pm variable.\n",
        "### Explore RainToday Variable"
      ],
      "metadata": {
        "id": "u3bgzMqAQPB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainToday'].nunique()"
      ],
      "metadata": {
        "id": "VqUiZWKCQHY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainToday'].unique()"
      ],
      "metadata": {
        "id": "r1k3SfK6QfgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df['RainToday'],dummy_na = True).sum()"
      ],
      "metadata": {
        "id": "UZBfpGP6Qgws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1406 null vlaues in raintoday variable\n",
        "### Explore RainTomorrow Variable"
      ],
      "metadata": {
        "id": "nhyzGKMqQrDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].nunique()"
      ],
      "metadata": {
        "id": "oMUP1BVGQoU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['RainTomorrow'].unique()"
      ],
      "metadata": {
        "id": "YLI0GnzLQxMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.get_dummies(df['RainTomorrow'],dummy_na = True).sum()"
      ],
      "metadata": {
        "id": "UKAFm6g5RBnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no null values for the rainTomorrow variable.\n",
        "\n",
        "## Explore Numerical Varibales"
      ],
      "metadata": {
        "id": "Ydd8OHnPRHTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the numerical variables\n",
        "numerical = [var for var in df.columns if df[var].dtype == 'float64']\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "print('The numerical variables are :\\n\\n', numerical)"
      ],
      "metadata": {
        "id": "O5APcFTNRGTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[numerical].head()"
      ],
      "metadata": {
        "id": "ixr6qIOxRuVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary of numerical variables\n",
        "* There are 16 numerical variables.\n",
        "* These are given by Mintemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustSpeed, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, and Temp3pm\n",
        "* All of the numerical variables are of continuous type.\n",
        "#### Explore problems within numerical varibales\n",
        "Now lets explore the numerical variables.\n",
        "#### Missing Values in numerical varibales"
      ],
      "metadata": {
        "id": "9oGmSP56VNdS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check missing values in numerical variables\n",
        "df[numerical].isna().sum()"
      ],
      "metadata": {
        "id": "IOBvgW9GVKfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that 16 of the numerical varibales contains null vlaues"
      ],
      "metadata": {
        "id": "i3Ftmn0Pa3mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View the summary statistics in numerical variables\n",
        "round(df[numerical].describe(),2)"
      ],
      "metadata": {
        "id": "S-gnHD1Iao-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On closer inspection, we can see that the Rainfall, Evaporation, windspeed9am and windspeed3pm columns may contain outliers.\n",
        "\n",
        "Lets draw boxplots to visualise outliers in teh above variables."
      ],
      "metadata": {
        "id": "ytUMOZB3bV7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw boxplots to visualize outliers\n",
        "plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "fig = sns.boxplot(df['Rainfall'])\n",
        "plt.title('Rainfall')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "fig = sns.boxplot(df['Evaporation'])\n",
        "plt.title('Evaporation')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "fig = sns.boxplot(df['WindSpeed9am'])\n",
        "plt.title('WindSpeed9am')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "fig = sns.boxplot(df['WindSpeed3pm'])\n",
        "plt.title('WindSpeed3pm')\n"
      ],
      "metadata": {
        "id": "x4Up77RkbEE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above boxplots confirm that there are alot of outliers in these variables.\n",
        "\n",
        "### Check the distribution of variables\n",
        "* Now, lets plot the histogram to check distributinos to find out if they are normal or skewed.\n",
        "* If the variable follows normal distribution, then we will do EXTREME VALUE ANALYSIS otherwise if they are skewed, We will find IQR (Interquantile range)."
      ],
      "metadata": {
        "id": "FlIJSPHMcc5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram to check distribution\n",
        "plt.figure(figsize = (15,10))\n",
        "\n",
        "plt.subplot(2,2,1)\n",
        "fig = df.Rainfall.hist(bins=5)\n",
        "fig.set_xlabel('Rainfall')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "plt.subplot(2,2,2)\n",
        "fig = df.Evaporation.hist(bins=5)\n",
        "fig.set_xlabel('Evaporation')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "plt.subplot(2,2,3)\n",
        "fig = df.WindSpeed9am.hist(bins=5)\n",
        "fig.set_xlabel('WindSpeed9am')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n",
        "plt.subplot(2,2,4)\n",
        "fig = df.WindSpeed3pm.hist(bins=5)\n",
        "fig.set_xlabel('WindSpeed3pm')\n",
        "fig.set_ylabel('RainTomorrow')\n",
        "\n"
      ],
      "metadata": {
        "id": "zAihVXapcUdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all the four variables are skewed. SO, we will use Interquantile range to find outliers."
      ],
      "metadata": {
        "id": "xHnJiUc1eLFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find outliers for Rainfall variables\n",
        "\n",
        "IQR = df['Rainfall'].quantile(0.75) - df['Rainfall'].quantile(0.25)\n",
        "Lower_fence = df['Rainfall'].quantile(0.25) - (3*IQR)\n",
        "Upper_fence = df['Rainfall'].quantile(0.75) + (3*IQR)\n",
        "\n",
        "print('Rainfall outliers are values {lowerbound} or {upperbound}'.format(lowerbound = Lower_fence, upperbound = Upper_fence))\n"
      ],
      "metadata": {
        "id": "bJAq2JSYcQIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Rainfall, the minimum and maximum values are 0.0 and 371.0, So, the outliers are values >3.2"
      ],
      "metadata": {
        "id": "5-Q8ZWOof20F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding outliers for Evaporation variable\n",
        "\n",
        "IQR = df['Evaporation'].quantile(0.75) - df['Evaporation'].quantile(0.25)\n",
        "Lower_fence = df['Evaporation'].quantile(0.25) - (3*IQR)\n",
        "Upper_fence = df['Evaporation'].quantile(0.75)  + (3*IQR)\n",
        "print('Evaporation outliers are values {lowerbound} or {upperbound}'.format(lowerbound = Lower_fence, upperbound = Upper_fence))"
      ],
      "metadata": {
        "id": "QLE3dAs1fT0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Evaporation, the minimum and maximum values are 0.0 and 145.0. So, the outliers are values>21.8"
      ],
      "metadata": {
        "id": "yymrnTpSg9uD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find outliers for Windspeed9am variable\n",
        "\n",
        "IQR = df['WindSpeed9am'].quantile(0.75) - df['WindSpeed9am'].quantile(0.25)\n",
        "Lower_fence = df['WindSpeed9am'].quantile(0.25) - (3*IQR)\n",
        "Upper_fence = df['WindSpeed9am'].quantile(0.75) + (3*IQR)\n",
        "print('WindSpeed9am outliers are values {lowerbound} or {upperbound}'.format(lowerbound = Lower_fence, upperbound = Upper_fence))"
      ],
      "metadata": {
        "id": "gCRlTAOkgdXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For windspeed9am, the minimum and maximum values are 0.0 and 130.o. So, the outliers are values >55.0"
      ],
      "metadata": {
        "id": "PjS93AEjhRkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find outliers for WInspeed3pm variable\n",
        "\n",
        "IQR = df['WindSpeed3pm'].quantile(0.75) - df['WindSpeed3pm'].quantile(0.25)\n",
        "Lower_fence = df['WindSpeed3pm'].quantile(0.25) - (3*IQR)\n",
        "Upper_fence = df['WindSpeed3pm'].quantile(0.75) + (3*IQR)\n",
        "print('WindSpeed3pm outliers are values {lowerbound} or {upperbound}'.format(lowerbound = Lower_fence, upperbound = Upper_fence))"
      ],
      "metadata": {
        "id": "yp-UuG6ThQrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For windspeed3pm the min and max values are 0.0 and 87.0. So,the outliers are values > 57.0"
      ],
      "metadata": {
        "id": "zcZFUXXwhpbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multivariate Analysis\n",
        "* An important step in EDA is to discover patterns and relationships between variables in the dataset.\n",
        "* We will use heat map and pair plot to discover the patterns and relationships in the dataset.\n",
        "* First of all, Lets draw a heatmap."
      ],
      "metadata": {
        "id": "Ch6jcMUNhyJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "T5ZRtpfFjLsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[numerical].corr()"
      ],
      "metadata": {
        "id": "tZQGs8cMhoJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (15,10))\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "sns.heatmap(df[numerical].corr(), annot = True)"
      ],
      "metadata": {
        "id": "3HoWBAdoi5Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpretation\n",
        "From the above correlation heat map, we can conclude that:-\n",
        "* MinTemp and MaxTemp variables are highly positively correlated(correlation coefficient = 0.75)\n",
        "* MinTemp and Temp3pm variables are also highly positively correlated (correlation coefficient  = 0.90)\n",
        "*"
      ],
      "metadata": {
        "id": "3fOcSg3Wmew6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pair Plot\n",
        "First of all, Lets define extract the varibales which are highly positively correlated."
      ],
      "metadata": {
        "id": "2hU1I9LNp8Ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_var = ['MinTemp','MaxTemp','Temp3pm','Temp9am',\"WindGustSpeed\",'WindSpeed3pm','Pressure9am',\"Pressure3pm\"]"
      ],
      "metadata": {
        "id": "aVjHRnzmniV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets draw pairplot to depict relationship between these variables."
      ],
      "metadata": {
        "id": "j4jd9lK2qeIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df[num_var])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35K8WeaXqcOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ??? HOw to read the heat map and pair plot...??"
      ],
      "metadata": {
        "id": "h4bCRt9xrl3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Declare feature vector and target variable"
      ],
      "metadata": {
        "id": "qBIkxW9grqyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(['RainTomorrow'], axis = 1)\n",
        "y = df['RainTomorrow']"
      ],
      "metadata": {
        "id": "KYXqIOpbqvl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Split data into separate training and test set"
      ],
      "metadata": {
        "id": "yrd2K8kKr3Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split X and Y into training and traingin sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2, random_state = 0)"
      ],
      "metadata": {
        "id": "7nR2B7fCrzmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape of X_train and X_test\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "AZhEvvSvslUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t-AMKAKpsvlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}